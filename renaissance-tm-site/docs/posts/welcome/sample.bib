
@misc{batmanghelich2018,
	address = {Boston University},
	title = {10-708 ({CMU}) {Probabilistic} {Graphical} {Models}: {CRF} + {Intro} to {Topic} {Models}},
	shorttitle = {Intro to {Topic} {Models}},
	url = {https://www.batman-lab.com/teaching/},
	language = {en},
	urldate = {2023-09-28},
	author = {Batmanghelich, Kayhan and Gormley, Matt},
	year = {2018},
	file = {Batmanghelich et al. - 11  CRF + Intro to Topic Models.pdf:/Users/call/Zotero/storage/PIH5JL6Z/Batmanghelich et al. - 11  CRF + Intro to Topic Models.pdf:application/pdf},
}

@misc{gao2020,
	title = {The {Pile}: {An} {800GB} {Dataset} of {Diverse} {Text} for {Language} {Modeling}},
	shorttitle = {The {Pile}},
	url = {http://arxiv.org/abs/2101.00027},
	doi = {10.48550/arXiv.2101.00027},
	abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present {\textbackslash}textit\{the Pile\}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
	month = dec,
	year = {2020},
	note = {arXiv:2101.00027 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/call/Zotero/storage/IITTKTWY/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/call/Zotero/storage/NYWHNW93/2101.html:text/html},
}

@inproceedings{goldberg2013,
	address = {Atlanta, Georgia, USA},
	title = {A {Dataset} of {Syntactic}-{Ngrams} over {Time} from a {Very} {Large} {Corpus} of {English} {Books}},
	url = {https://aclanthology.org/S13-1035},
	urldate = {2023-09-28},
	booktitle = {Second {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM}), {Volume} 1: {Proceedings} of the {Main} {Conference} and the {Shared} {Task}: {Semantic} {Textual} {Similarity}},
	publisher = {Association for Computational Linguistics},
	author = {Goldberg, Yoav and Orwant, Jon},
	month = jun,
	year = {2013},
	pages = {241--247},
	file = {Full Text PDF:/Users/call/Zotero/storage/Y5FPNVX2/Goldberg and Orwant - 2013 - A Dataset of Syntactic-Ngrams over Time from a Ver.pdf:application/pdf},
}

@article{blei2012,
	title = {Topic {Modeling} and {Digital} {Humanities}},
	volume = {2},
	url = {https://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/},
	number = {1},
	urldate = {2023-09-28},
	journal = {Journal of Digital Humanities},
	author = {Blei, David},
	year = {2012},
	file = {Snapshot:/Users/call/Zotero/storage/73EE4I7T/topic-modeling-and-digital-humanities-by-david-m-blei.html:text/html},
}
